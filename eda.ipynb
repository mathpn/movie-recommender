{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data. First, we'll use ratings to create a collaborative filtering algorithm. Then, we'll use movie metadata to find similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata = pd.read_csv('./data/movies_metadata.csv')\n",
    "ratings = pd.read_csv('./data/ratings_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = movies_metadata['vote_average'].mean()\n",
    "m = movies_metadata['vote_count'].quantile(0.9)\n",
    "\n",
    "q_movies = movies_metadata.copy().loc[movies_metadata['vote_count'] >= m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)\n",
    "q_movies = q_movies.sort_values('score', ascending=False)\n",
    "\n",
    "q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./data/ratings_small.csv\")\n",
    "print(ratings.head())\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ratings[['userId']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "svd = SVD()\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [svd.predict(1, i).est for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: new users or new ratings by existing users are expected to come in quickly. It's not really feasible to retrain the entire model each time a new ratings comes in. Unfortunately, common implementations of SVD of even KNN-based collaborative filtering algorithms do not support online-learning for new ratings without retraining the whole model.\n",
    "\n",
    "For this, I'll try to implement the online-updating algorithm presented in: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.8010&rep=rep1&type=pdf\n",
    "\n",
    "The implementation will follow default parameters from the surprise package: https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "values = ratings.loc[:, 'rating'].to_numpy()\n",
    "rows = ratings.loc[:, 'userId'].to_numpy()\n",
    "cols = ratings.loc[:, 'movieId'].to_numpy()\n",
    "\n",
    "n_rows = rows.max() + 1\n",
    "n_cols = cols.max() + 1\n",
    "\n",
    "sparse_ratings = sparse.csr_matrix((values, (rows - 1, cols - 1)), shape=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class KMF(nn.Module):\n",
    "    def __init__(self, n_users: int, n_items: int, emb_dim: int, max_score: int):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.user_bias = nn.Parameter(torch.zeros(n_users))\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.item_bias = nn.Parameter(torch.zeros(n_items))\n",
    "        nn.init.normal_(self.user_emb.weight, 0, 0.1)\n",
    "        nn.init.normal_(self.item_emb.weight, 0, 0.1)\n",
    "        self.max_score = max_score\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        users_emb = self.user_emb(users)\n",
    "        items_emb = self.item_emb(items)\n",
    "        user_bias = self.user_bias[users]\n",
    "        item_bias = self.item_bias[items]\n",
    "        return self.max_score * torch.sigmoid(user_bias + item_bias + (items_emb * users_emb).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def mse(scores, pred):\n",
    "    return (scores - pred).pow(2).mean() + alpha * (model.user_emb.weight.pow(2).sum() + model.item_emb.weight.pow(2).sum())\n",
    "\n",
    "\n",
    "def get_param_squared_norms(model, include_bias: bool = False):\n",
    "    emb_norms = model.user_emb.weight.pow(2).sum() + model.item_emb.weight.pow(2).sum()\n",
    "    # XXX problem: logistic output needs high bias norms\n",
    "    bias_norms = (\n",
    "        (model.user_bias - model.user_bias.mean()).pow(2).sum()\n",
    "        + (model.item_bias - model.item_bias.mean()).pow(2).sum()\n",
    "    )\n",
    "    if include_bias:\n",
    "        return emb_norms + bias_norms\n",
    "    return emb_norms\n",
    "\n",
    "\n",
    "n_users = ratings.loc[:, 'userId'].max()\n",
    "n_items = ratings.loc[:, 'movieId'].max()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = KMF(n_users, n_items, 100, 5)\n",
    "model.to(device)\n",
    "\n",
    "dataset = [\n",
    "    (torch.tensor([value, row - 1, col - 1]).float().to(device))\n",
    "    for value, row, col in zip(values, rows, cols)\n",
    "]\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split_point = int(0.8 * len(dataset))\n",
    "train_dataset = dataset[:split_point]\n",
    "test_dataset = dataset[split_point:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "alpha = 0.02\n",
    "\n",
    "for epoch in range(20):\n",
    "    mse_acc = l2_acc = steps = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        scores = batch[:, 0]\n",
    "        users = batch[:, 1].long()\n",
    "        items = batch[:, 2].long()\n",
    "        pred = model(users, items)\n",
    "        mse_loss = mse(scores, pred)\n",
    "        l2_loss = alpha * get_param_squared_norms(model)\n",
    "        mse_acc += float(mse_loss)\n",
    "        l2_acc += float(l2_loss)\n",
    "        loss = mse_loss + l2_loss\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        optimizer.step()\n",
    "        if i and i % 50 == 0:\n",
    "            mse_acc /= steps\n",
    "            l2_acc /= steps\n",
    "            print(f\"MSE = {mse_acc:.3f} | L2 reg = {l2_acc:.2f}\")\n",
    "            mse_acc = l2_acc = steps = 0\n",
    "    test_loss = test_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            scores = batch[:, 0]\n",
    "            users = batch[:, 1].long()\n",
    "            items = batch[:, 2].long()\n",
    "            pred = model(users, items)\n",
    "            mse_loss = mse(scores, pred)\n",
    "            test_loss += mse_loss\n",
    "            test_steps += 1\n",
    "    print(f\"Test: MSE = {test_loss / test_steps:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: implement steps to add/update users and check if it's too slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9db47ae5badfead9fd39188fd18ff2ce368bc487d9efee61e4db9417ef6acdbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
